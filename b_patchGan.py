import numpy as npimport tensorflow as tfimport pandas as pdimport sklearn.ensemble.voting_classifier#import tensorflow_probability as tfpfrom collections import OrderedDict, defaultdictfrom bgan_util import *from dcgan_ops import *def conv_out_size(size, stride):    return int(math.ceil(size / float(stride)))def kernel_sizer(size, stride):    ko = int(math.ceil(size / float(stride)))    if ko % 2 == 0:        ko += 1    return koclass B_PATCHGAN(object):    def __init__(self, x_dim, batch_size=64, gf_dim=64, df_dim=64,                 prior_std=1.0, J=1, alpha=0.01, lr=0.005, optimizer='adam', wasserstein=False, num_classes=2,                 ml=False, num_train=64, l1lambda=0.1):  # eta=2e-4,        assert len(x_dim) == 3, "invalid image dims"        self.is_grayscale = (x_dim[2] == 1)        self.optimizer = optimizer.lower()        self.batch_size = batch_size        self.num_train = num_train        self.n_classes = num_classes        self.x_dim = x_dim        self.gf_dim = gf_dim        self.df_dim = df_dim        self.c_dim = x_dim[2]        self.lr = lr        self.l1_lambda = l1lambda        # Bayes        self.prior_std = prior_std   # sigma        self.num_gen = J  # what is num_gen ??        self.num_disc = 1        self.num_mcmc = 1  # M        self.alpha = alpha        self.noise_std = np.sqrt(2 * self.alpha * self.lr)  # self.eta = self.lr as default | 10        # ML        self.ml = ml        if self.ml:            assert self.num_gen == 1 and self.num_disc == 1 and self.num_mcmc == 1, "invalid settings for ML training"        def get_strides(num_layers, num_pool):            interval = int(math.floor(num_layers / float(num_pool)))            strides = np.array([1] * num_layers)            strides[0:interval * num_pool:interval] = 2            return strides        # generator : number of pooling 4 pool + 4 unpool        self.num_pool = 4        num_layers = 4        self.gen_strides = get_strides(num_layers, self.num_pool)        self.disc_strides = self.gen_strides        num_dfs = np.cumprod(np.array([self.df_dim] + list(self.disc_strides)))[:-1]        self.num_dfs = list(num_dfs)        self.num_gfs = self.num_dfs  # [::-1] - not reverse        self.construct_from_hypers()        self.build_bpgan_graph()    def construct_from_hypers(self, gen_kernel_size=3, num_dfs=None, num_gfs=None):        self.num_disc_layers = 5        self.num_gen_layers = 19        self.d_batch_norm = AttributeDict(            [("d_bn%i" % dbn_i, batch_norm(name='d_bn%i' % dbn_i)) for dbn_i in range(self.num_disc_layers)])        self.sup_g_batch_norm = AttributeDict(            [("sup_g_bn%i" % gbn_i, batch_norm(name='sup_g_bn%i' % gbn_i)) for gbn_i in range(self.num_gen_layers)])        self.g_batch_norm = AttributeDict(            [("g_bn%i" % gbn_i, batch_norm(name='g_bn%i' % gbn_i)) for gbn_i in range(self.num_gen_layers)])        if num_dfs is None:            num_dfs = [self.df_dim, self.df_dim * 2, self.df_dim * 4, self.df_dim * 8, self.df_dim]        if num_gfs is None:            num_gfs = [self.gf_dim, self.gf_dim, self.gf_dim * 2, self.gf_dim * 2,                       self.gf_dim * 4, self.gf_dim * 4, self.gf_dim * 8, self.gf_dim * 8,                       self.gf_dim * 16, self.gf_dim * 16,                       self.gf_dim * 8, self.gf_dim * 8,                       self.gf_dim * 4, self.gf_dim * 4,                       self.gf_dim * 2, self.gf_dim * 2,                       self.gf_dim, self.gf_dim,                       self.n_classes]  # the number of the classes for the last conv layer ??        s_h, s_w = self.x_dim[0], self.x_dim[1]        s_h2, s_w2 = conv_out_size(s_h, 2), conv_out_size(s_w, 2)        s_h4, s_w4 = conv_out_size(s_h2, 2), conv_out_size(s_w2, 2)        s_h8, s_w8 = conv_out_size(s_h4, 2), conv_out_size(s_w4, 2)        s_h16, s_w16 = conv_out_size(s_h8, 2), conv_out_size(s_w8, 2)        ks = gen_kernel_size        self.gen_weight_dims = OrderedDict()        num_gfs = num_gfs + [self.c_dim]        self.gen_kernel_sizes = [ks]        #### build unet_generator from the one-by-one        self.gen_weight_dims["g_h%i_W" % 0] = (3, 3, 1, num_gfs[0])  # from the image        self.gen_weight_dims["g_h%i_b" % 0] = (num_gfs[0],)        self.gen_weight_dims["g_h%i_W" % 1] = (3, 3, num_gfs[1], num_gfs[1])  # conv1        self.gen_weight_dims["g_h%i_b" % 1] = (num_gfs[1],)        # self.gen_weight_dims["g_h%i_W" % 20] = (1, 1, 1, num_gfs[1])  # shortcut_1        # self.gen_weight_dims["g_h%i_b" % 20] = (num_gfs[1],)  # shortcut_1        self.gen_weight_dims["g_h%i_W" % 2] = (3, 3, num_gfs[1], num_gfs[2])        self.gen_weight_dims["g_h%i_b" % 2] = (num_gfs[2],)        self.gen_weight_dims["g_h%i_W" % 3] = (3, 3, num_gfs[3], num_gfs[3])  # conv2        self.gen_weight_dims["g_h%i_b" % 3] = (num_gfs[3],)        # self.gen_weight_dims["g_h%i_W" % 21] = (1, 1, num_gfs[1], num_gfs[2])  # shortcut_2        # self.gen_weight_dims["g_h%i_b" % 21] = (num_gfs[2],)  # shortcut_2        self.gen_weight_dims["g_h%i_W" % 4] = (3, 3, num_gfs[3], num_gfs[4])        self.gen_weight_dims["g_h%i_b" % 4] = (num_gfs[4],)        self.gen_weight_dims["g_h%i_W" % 5] = (3, 3, num_gfs[5], num_gfs[5])  # conv3        self.gen_weight_dims["g_h%i_b" % 5] = (num_gfs[5],)        # self.gen_weight_dims["g_h%i_W" % 22] = (1, 1, num_gfs[3], num_gfs[4])  # shortcut_3        # self.gen_weight_dims["g_h%i_b" % 22] = (num_gfs[4],)  # shortcut_3        self.gen_weight_dims["g_h%i_W" % 6] = (3, 3, num_gfs[5], num_gfs[6])        self.gen_weight_dims["g_h%i_b" % 6] = (num_gfs[6],)        self.gen_weight_dims["g_h%i_W" % 7] = (3, 3, num_gfs[7], num_gfs[7])  # conv4        self.gen_weight_dims["g_h%i_b" % 7] = (num_gfs[7],)        # self.gen_weight_dims["g_h%i_W" % 23] = (1, 1, num_gfs[5], num_gfs[6])  # shortcut_4        # self.gen_weight_dims["g_h%i_b" % 23] = (num_gfs[6],)  # shortcut_4        self.gen_weight_dims["g_h%i_W" % 8] = (3, 3, num_gfs[7], num_gfs[8])        self.gen_weight_dims["g_h%i_b" % 8] = (num_gfs[8],)        self.gen_weight_dims["g_h%i_W" % 9] = (3, 3, num_gfs[9], num_gfs[9])  # conv5        self.gen_weight_dims["g_h%i_b" % 9] = (num_gfs[9],)        # self.gen_weight_dims["g_h%i_W" % 24] = (1, 1, num_gfs[7], num_gfs[8])  # shortcut_5        # self.gen_weight_dims["g_h%i_b" % 24] = (num_gfs[8],)  # shortcut_5        self.gen_weight_dims["g_h%i_W" % 10] = (3, 3, num_gfs[9] + num_gfs[7], num_gfs[10])  # conv6 concat conv4        self.gen_weight_dims["g_h%i_b" % 10] = (num_gfs[10],)        self.gen_weight_dims["g_h%i_W" % 11] = (3, 3, num_gfs[11], num_gfs[11])        self.gen_weight_dims["g_h%i_b" % 11] = (num_gfs[11],)        # self.gen_weight_dims["g_h%i_W" % 25] = (1, 1, num_gfs[9] + num_gfs[7], num_gfs[11])  # shortcut_6        # self.gen_weight_dims["g_h%i_b" % 25] = (num_gfs[11],)  # shortcut_6        self.gen_weight_dims["g_h%i_W" % 12] = (3, 3, num_gfs[11] + num_gfs[5], num_gfs[12])  # conv7 concat conv3        self.gen_weight_dims["g_h%i_b" % 12] = (num_gfs[12],)        self.gen_weight_dims["g_h%i_W" % 13] = (3, 3, num_gfs[13], num_gfs[13])        self.gen_weight_dims["g_h%i_b" % 13] = (num_gfs[13],)        # self.gen_weight_dims["g_h%i_W" % 26] = (1, 1, num_gfs[11] + num_gfs[5], num_gfs[13])  # shortcut_7        # self.gen_weight_dims["g_h%i_b" % 26] = (num_gfs[13],)  # shortcut_7        self.gen_weight_dims["g_h%i_W" % 14] = (3, 3, num_gfs[13] + num_gfs[3], num_gfs[14])  # conv8 concat conv2        self.gen_weight_dims["g_h%i_b" % 14] = (num_gfs[14],)        self.gen_weight_dims["g_h%i_W" % 15] = (3, 3, num_gfs[15], num_gfs[15])        self.gen_weight_dims["g_h%i_b" % 15] = (num_gfs[15],)        # self.gen_weight_dims["g_h%i_W" % 27] = (1, 1, num_gfs[13] + num_gfs[3], num_gfs[15])  # shortcut_8        # self.gen_weight_dims["g_h%i_b" % 27] = (num_gfs[15],)  # shortcut_8        self.gen_weight_dims["g_h%i_W" % 16] = (3, 3, num_gfs[15] + num_gfs[1], num_gfs[16])  # conv9 concat conv1        self.gen_weight_dims["g_h%i_b" % 16] = (num_gfs[16],)        self.gen_weight_dims["g_h%i_W" % 17] = (3, 3, num_gfs[17], num_gfs[17])        self.gen_weight_dims["g_h%i_b" % 17] = (num_gfs[17],)        # self.gen_weight_dims["g_h%i_W" % 28] = (1, 1, num_gfs[15] + num_gfs[1], num_gfs[17])  # shortcut_9        # self.gen_weight_dims["g_h%i_b" % 28] = (num_gfs[17],)  # shortcut_9        ################### output layer #########################        self.gen_weight_dims["g_h%i_W" % 18] = (1, 1, num_gfs[17], num_gfs[18])        self.gen_weight_dims["g_h%i_b" % 18] = (num_gfs[18],)        ## for the log_variance        # self.gen_weight_dims["g_h%i_W" % 19] = (1, 1, num_gfs[17], num_gfs[18])        # self.gen_weight_dims["g_h%i_b" % 19] = (num_gfs[18],)#########################################################################################################        self.disc_weight_dims = OrderedDict()        self.disc_weight_dims["d_h%i_W" % 0] = (5, 5, self.c_dim + self.c_dim, num_dfs[0])  # output = ( s_h / 2, s_w / 2 )   + self.num_classes        self.disc_weight_dims["d_h%i_b" % 0] = (num_dfs[0],)        self.disc_weight_dims["d_h%i_W" % 1] = (5, 5, num_dfs[0], num_dfs[1])  # output = ( s_h / 4, s_w / 4 )        self.disc_weight_dims["d_h%i_b" % 1] = (num_dfs[1],)        self.disc_weight_dims["d_h%i_W" % 2] = (5, 5, num_dfs[1], num_dfs[2])  # output = ( s_h / 8, s_w / 8 )        self.disc_weight_dims["d_h%i_b" % 2] = (num_dfs[2],)        self.disc_weight_dims["d_h%i_W" % 3] = (5, 5, num_dfs[2], num_dfs[3])  # output = ( s_h / 16, s_w / 16 )   # pre: 1, 1,        self.disc_weight_dims["d_h%i_b" % 3] = (num_dfs[3],)        self.disc_weight_dims["d_h%i_W" % 4] = (1, 1, num_dfs[3], 1)        self.disc_weight_dims["d_h%i_b" % 4] = (1,)########################################################################################################################        self.distrib_weight_dims = OrderedDict()        for zi in range(self.num_gen):            self.distrib_weight_dims["dist_%i_mu" % zi] = (1, self.x_dim[0], self.x_dim[1], 1)            self.distrib_weight_dims["dist_%i_var" % zi] = (1, self.x_dim[0], self.x_dim[1], 1)########################################################################################################################        for k, v in self.gen_weight_dims.items():   # k is the name, v is the dim            print("gen_weight_dims - %s: %s" % (k, v))        print('****')        for k, v in self.disc_weight_dims.items():            print("dics_weight_dims - %s: %s" % (k, v))        print('****')        for k, v in self.distrib_weight_dims.items():            print("distrib_weight_dims - %s: %s" % (k, v))    def initialize_dist_wgts(self, scope_str):        if scope_str == "distrib_classifier":            weight_dims = self.distrib_weight_dims        else:            raise RuntimeError("invalid scope!")        param_list = []        with tf.variable_scope(scope_str) as scope:            wgts_ = AttributeDict()            for zi in range(self.num_gen):                mu_name = "dist_%i_mu" % (zi)                mu_shape = weight_dims[mu_name]                var_name = "dist_%i_var" % (zi)                var_shape = weight_dims[var_name]                wgts_[mu_name] = tf.get_variable("%s" % mu_name, mu_shape, initializer=tf.random_normal_initializer(mean=1/self.num_gen, stddev=0.02))                wgts_[var_name] = tf.get_variable("%s" % var_name, var_shape, initializer=tf.random_normal_initializer(stddev=0.02))            #for name, shape in weight_dims.items():             #   wgts_[name] = tf.get_variable("%s" % name, shape, initializer=tf.random_normal_initializer(mean=1/self.num_gen, stddev=0.02))            param_list.append(wgts_)            return param_list    def initialize_wgts(self, scope_str):        if scope_str == "generator" or scope_str == "res_generator":            weight_dims = self.gen_weight_dims  # len(weight_dims) = 10 / 2            numz = self.num_gen        elif scope_str == "patch_discriminator":            weight_dims = self.disc_weight_dims  # len(weight_dims) = 10 / 2            numz = self.num_disc        else:            raise RuntimeError("invalid scope!")        param_list = []        with tf.variable_scope(scope_str) as scope:  # iterated J (numz / num_gen) x num_mcmc = 20            for zi in range(numz):  # numz: num_gen / num_disc                for m in range(self.num_mcmc):                    wgts_ = AttributeDict()                    for name, shape in weight_dims.items():                        wgts_[name] = tf.get_variable("%s_%04d_%04d" % (name, zi, m), shape,                                                      initializer=tf.random_normal_initializer(stddev=0.02))                    param_list.append(wgts_)            return param_list    ########################################################################################################################    def build_bpgan_graph(self):        data_pair = tf.placeholder(tf.float32, [self.batch_size, self.x_dim[0], self.x_dim[1], self.c_dim + self.c_dim], name='stacked_data_pair')   # + n_classes, shape = [ self.batch_size        self.imgs = data_pair[:, :, :, self.c_dim: self.c_dim + self.c_dim]   #+ self.n_classes        self.labels = data_pair[:, :, :, : self.c_dim]        self.pos_pair = tf.concat([self.imgs, self.labels], axis=3)        self.gen_param_list = self.initialize_wgts("generator")   # res_generator        self.disc_param_list = self.initialize_wgts("patch_discriminator")        self.distrib_param_list = self.initialize_dist_wgts("distrib_classifier")        self.d_semi_learning_rate = tf.placeholder(tf.float32, shape=[])        self.g_learning_rate = tf.placeholder(tf.float32, shape=[])        t_vars = tf.trainable_variables()  # compile all disciminative weights  # returns a list of trainable variables        self.d_vars = []        self.d_vars.append([var for var in t_vars if 'd_' in var.name and "_%04d_%04d" % (0, 0) in var.name])  # var for var in t_vars if 'd_' in var.name        self.g_vars = []        for gi in range(self.num_gen):            for m in range(self.num_mcmc):                self.g_vars.append([var for var in t_vars if 'g_' in var.name and "_%04d_%04d" % (gi, m) in var.name])        self.dist_vars = []        self.dist_vars.append([var for var in t_vars if 'dist_' in var.name])        self.d_losses, self.d_optims_adam = [], []        self.g_losses, self.g_optims_adam = [], []        self.distrib_loss = []        self.g_seg_losses = []        self.predicts = []        #self.g_aleatoric_losses = []        #self.variances = []        ############################ build discrimitive losses and optimizers ##########################################        d_probs_pos_real, d_logits_pos_real, d_feature_real = self.patch_discriminator(self.pos_pair, self.disc_param_list[0])  # self.pos_pair, self.disc_param_list[0]        d_loss_real = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=d_logits_pos_real, labels=tf.ones_like(d_probs_pos_real)))#        tf.summary.histogram("d_real", d_loss_real)        #######################################################        ### Part II: fake - multi generators, multi fake losses ###        d_loss_fakes = []        d_losses_semi = []        for gi, gen_params in enumerate(self.gen_param_list):            gi_losses = []            ###################### Predicts = self.generator(self.imgs, gen_params)  # , pred_sigmoid ##################            logits = self.generator(self.imgs, gen_params)   # res_generator            predicts = tf.nn.relu(logits)            self.predicts.append(predicts)            #log_var = tf.nn.softplus(log_var)            #self.variances.append(log_var)            ################ Segmentation Loss ####################            seg_loss_dice = 1. - dice_coef(predicts, self.labels)            seg_loss_cross = tf.reduce_mean(tf.nn.weighted_cross_entropy_with_logits(logits=tf.nn.sigmoid(logits), targets=self.labels, pos_weight=0.25))            ########### Compute Aleatoric Loss ####################            #aleatoric_loss = tf.reduce_mean(tf.exp(-log_var) * tf.square(tf.nn.softplus(logits) - self.labels)) + tf.reduce_mean(log_var)            #aleatoric_loss = tf.reduce_mean(tf.exp(-log_var) * (seg_loss_dice + seg_loss_cross) / 2 + tf.reduce_mean(log_var))            #aleatoric_loss = seg_loss_cross + seg_loss_dice            #######################################################            fake_AB = tf.concat([self.imgs, predicts], axis=3)            with tf.device("/gpu:0"):                d_probs_fake_, d_logits_fake_, d_feature_fake_ = self.patch_discriminator(fake_AB, self.disc_param_list[0])                d_loss_fake_pos = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=d_logits_fake_, labels=tf.zeros_like(d_probs_fake_)))                d_loss_fakes.append(d_loss_fake_pos)                g_loss_fake = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=d_logits_fake_, labels=tf.ones_like(d_probs_fake_)))        #logits=tf.log(tf.clip_by_value(d_logits_fake_,1e-10,1.0))                #g_huber_loss = tf.reduce_mean(huber_loss(d_feature_real[-1], d_feature_fake_[-1]))                g_loss_ = g_loss_fake # + g_huber_loss  # add seg_loss here                if not self.ml:                    g_loss_ += self.gen_prior(gen_params) + self.gen_noise(gen_params)  # return the prior_loss + noise_loss                gi_losses.append(tf.reshape(g_loss_, [1]))                g_loss = tf.reduce_sum(tf.concat(gi_losses, 0))  # tf.reduce_logsumexp(tf.concat(gi_losses, 0))                ################# seg loss #########################################################################################                g_loss += self.l1_lambda * seg_loss_dice + seg_loss_cross                ####################################################################################################################            with tf.device("/gpu:1"):                self.g_losses.append(g_loss)                self.g_seg_losses.append(seg_loss_dice)                #self.g_aleatoric_losses.append(aleatoric_loss)#                tf.summary.histogram("g_loss_fake_%d" % gi, self.g_losses)#                tf.summary.histogram("g_seg_%d" % gi, self.g_seg_losses)                g_optimizer = tf.train.AdamOptimizer(learning_rate=self.g_learning_rate, beta1=0.5)                self.g_optims_adam.append(g_optimizer.minimize(g_loss, var_list=self.g_vars[gi]))        #### out of the generator loop ####        with tf.device("/gpu:2"):            for d_loss_fake_ in d_loss_fakes:                d_loss_semi_ = d_loss_real + d_loss_fake_ * float(self.num_gen)                d_losses_semi.append(tf.reshape(d_loss_semi_, [1]))            d_loss_semi = tf.reduce_sum(tf.concat(d_losses_semi, 0))            self.d_losses.append(d_loss_semi)#            tf.summary.histogram("d_loss_fake", self.d_losses)            # default iterations            d_optimizer = tf.train.AdamOptimizer(learning_rate=self.d_semi_learning_rate, beta1=0.5)            self.d_optims_adam.append(d_optimizer.minimize(d_loss_semi, var_list=self.d_vars[0]))            ############################################################################################################            self.gen_fused, exp_vars, log_vars = self.distrib_classifier(self.predicts, self.distrib_param_list[0])   #            fused_loss_dice = 1.0 - dice_coef(self.labels, self.gen_fused)            fused_loss = tf.multiply(exp_vars, fused_loss_dice) + log_vars            self.distrib_loss.append(fused_loss_dice)   #  tf.reduce_mean(tf.squared_difference(self.gen_fused, self.labels))  # tf.squared_difference(self.gen_fused, self.labels)            dist_optimizer = tf.train.AdamOptimizer(learning_rate=self.g_learning_rate, beta1=0.5)            self.dist_optims_adam = []            self.dist_optims_adam.append(dist_optimizer.minimize(fused_loss, var_list=self.dist_vars[0]))            ############################################################################################################    def distrib_classifier(self, image_list, dist_params):        with tf.variable_scope("distrib_classifier", reuse=tf.AUTO_REUSE) as scope:            self.weighted_maps = []            dist_0 = tf.distributions.Normal(loc=dist_params["dist_%i_mu" % 0], scale=dist_params["dist_%i_var" % 0]).sample()            y = tf.multiply(image_list[0], dist_0)            self.weighted_maps.append(tf.multiply(image_list[0], dist_params["dist_%i_mu" % 0]))            log_vars = dist_params["dist_%i_var" % 0]            exp_vars = tf.exp(- dist_params["dist_%i_var" % 0])            for zi in range(1, self.num_gen):                dist_zi = tf.distributions.Normal(loc=dist_params["dist_%i_mu" % zi], scale=dist_params["dist_%i_var" % zi]).sample()                y = tf.add(y, tf.multiply(image_list[zi],dist_zi))   # dist_params["dist_%i_mu" % zi]                self.weighted_maps.append(tf.multiply(image_list[zi], dist_params["dist_%i_mu" % zi]))                log_vars = tf.add(log_vars, dist_params["dist_%i_var" % zi])                exp_vars = tf.add(exp_vars, tf.exp( - dist_params["dist_%i_var" % zi]))            return y, exp_vars, log_vars    def patch_discriminator(self, image, disc_params):  # , disc_params        with tf.variable_scope("patch_discriminator", reuse=tf.AUTO_REUSE) as scope:            h0 = lrelu(conv2d(image, self.df_dim, name='d_h0_conv', w=disc_params["d_h%i_W" % 0], biases=disc_params["d_h%i_b" % 0]))            h1 = lrelu(self.d_batch_norm["d_bn%i" % 1](                conv2d(h0, self.df_dim * 2, name='d_h1_conv', w=disc_params["d_h%i_W" % 1],                       biases=disc_params["d_h%i_b" % 1])))            h2 = lrelu(self.d_batch_norm["d_bn%i" % 2](                conv2d(h1, self.df_dim * 4, name='d_h2_conv', w=disc_params["d_h%i_W" % 2],                       biases=disc_params["d_h%i_b" % 2])))            h3 = lrelu(self.d_batch_norm["d_bn%i" % 3](                conv2d(h2, self.df_dim * 8, name='d_h3_conv', w=disc_params["d_h%i_W" % 3],                       biases=disc_params["d_h%i_b" % 3])))  #  k_w=1, k_h=1,            h4 = self.d_batch_norm["d_bn%i" % 4](                conv2d(h3, 1, name='d_h4_conv', k_w=1, k_h=1, w=disc_params["d_h%i_W" % 4],                       biases=disc_params["d_h%i_b" % 4]))  #            return tf.nn.sigmoid(h4), h4, [h3]  # probs for each sample (image), logits    def generator(self, image, gen_params):        with tf.variable_scope("generator", reuse=tf.AUTO_REUSE) as scope:            conv1 = conv2d(image, self.gf_dim, k_h=3, k_w=3, d_h=1, d_w=1, name='conv1_conv1', w=gen_params["g_h%i_W" % 0], biases=gen_params["g_h%i_b" % 0])            conv1 = self.g_batch_norm["g_bn%i" % 0](conv1)            conv1 = tf.nn.relu(conv1, name='conv1_relu1')            conv1 = conv2d(conv1, self.gf_dim, k_h=3, k_w=3, d_h=1, d_w=1, name='conv1_conv2', w=gen_params["g_h%i_W" % 1], biases=gen_params["g_h%i_b" % 1])            conv1 = self.g_batch_norm["g_bn%i" % 1](conv1)            conv1 = tf.nn.relu(conv1, name='conv1_relu2')            pool1 = maxpooling_2d(conv1, name='maxpool2')            conv2 = conv2d(pool1, 2 * self.gf_dim, k_h=3, k_w=3, d_h=1, d_w=1, name='conv2_conv1', w=gen_params["g_h%i_W" % 2], biases=gen_params["g_h%i_b" % 2])            conv2 = self.g_batch_norm["g_bn%i" % 2](conv2)            conv2 = tf.nn.relu(conv2, name='conv2_relu1')            conv2 = conv2d(conv2, 2 * self.gf_dim, k_h=3, k_w=3, d_h=1, d_w=1, name='conv2_conv2', w=gen_params["g_h%i_W" % 3], biases=gen_params["g_h%i_b" % 3])            conv2 = self.g_batch_norm["g_bn%i" % 3](conv2)            conv2 = tf.nn.relu(conv2, name='conv2_relu2')            pool2 = maxpooling_2d(conv2, name='maxpool2')            conv3 = conv2d(pool2, 4 * self.gf_dim, k_h=3, k_w=3, d_h=1, d_w=1, name='conv3_conv1', w=gen_params["g_h%i_W" % 4], biases=gen_params["g_h%i_b" % 4])            conv3 = self.g_batch_norm["g_bn%i" % 4](conv3)            conv3 = tf.nn.relu(conv3, name='conv3_relu1')            conv3 = conv2d(conv3, 4 * self.gf_dim, k_h=3, k_w=3, d_h=1, d_w=1, name='conv3_conv2', w=gen_params["g_h%i_W" % 5], biases=gen_params["g_h%i_b" % 5])            conv3 = self.g_batch_norm["g_bn%i" % 5](conv3)            conv3 = tf.nn.relu(conv3, name='conv3_relu2')            pool3 = maxpooling_2d(conv3, name='maxpool3')            pool3 = tf.nn.dropout(pool3, .5)            conv4 = conv2d(pool3, 8 * self.gf_dim, k_h=3, k_w=3, d_h=1, d_w=1, name='conv4_conv1', w=gen_params["g_h%i_W" % 6], biases=gen_params["g_h%i_b" % 6])            conv4 = self.g_batch_norm["g_bn%i" % 6](conv4)            conv4 = tf.nn.relu(conv4, name='conv4_relu1')            conv4 = conv2d(conv4, 8 * self.gf_dim, k_h=3, k_w=3, d_h=1, d_w=1, name='conv4_conv2', w=gen_params["g_h%i_W" % 7], biases=gen_params["g_h%i_b" % 7])            conv4 = self.g_batch_norm["g_bn%i" % 7](conv4)            conv4 = tf.nn.relu(conv4, name='conv4_relu2')            pool4 = maxpooling_2d(conv4, name='maxpool4')            pool4 = tf.nn.dropout(pool4, .5)            conv5 = conv2d(pool4, 16 * self.gf_dim, k_h=3, k_w=3, d_h=1, d_w=1, name='conv5_conv1', w=gen_params["g_h%i_W" % 8], biases=gen_params["g_h%i_b" % 8])            conv5 = self.g_batch_norm["g_bn%i" % 8](conv5)            conv5 = tf.nn.relu(conv5, name='conv5_relu1')            conv5 = conv2d(conv5, 16 * self.gf_dim, k_h=3, k_w=3, d_h=1, d_w=1, name='conv5_conv2', w=gen_params["g_h%i_W" % 9], biases=gen_params["g_h%i_b" % 9])            conv5 = self.g_batch_norm["g_bn%i" % 9](conv5)            conv5 = tf.nn.relu(conv5, name='conv5_relu2')            conv5 = tf.nn.dropout(conv5, .5)            up1 = upsampling2d(conv5)            conv6 = tf.concat([up1, conv4], axis=3, name='conv6_concat')  # axis = 3 ??            conv6 = conv2d(conv6, 8 * self.gf_dim, k_h=3, k_w=3, d_h=1, d_w=1, name='conv6_conv1', w=gen_params["g_h%i_W" % 10], biases=gen_params["g_h%i_b" % 10])            conv6 = self.g_batch_norm["g_bn%i" % 10](conv6)            conv6 = tf.nn.relu(conv6, name='conv6_relu1')            conv6 = conv2d(conv6, 8 * self.gf_dim, k_h=3, k_w=3, d_h=1, d_w=1, name='conv6_conv2', w=gen_params["g_h%i_W" % 11], biases=gen_params["g_h%i_b" % 11])            conv6 = self.g_batch_norm["g_bn%i" % 11](conv6)            conv6 = tf.nn.relu(conv6, name='conv6_relu2')            conv6 = tf.nn.dropout(conv6, .5)            up2 = upsampling2d(conv6)  # up2 = tf_utils.upsampling3d(conv6, size=(2, 2, 2), name='conv7_up')            conv7 = tf.concat([up2, conv3], axis=3, name='conv7_concat')            conv7 = conv2d(conv7, 4 * self.gf_dim, k_h=3, k_w=3, d_h=1, d_w=1, name='conv7_conv1', w=gen_params["g_h%i_W" % 12], biases=gen_params["g_h%i_b" % 12])            conv7 = self.g_batch_norm["g_bn%i" % 12](conv7)            conv7 = tf.nn.relu(conv7, name='conv7_relu1')            conv7 = conv2d(conv7, 4 * self.gf_dim, k_h=3, k_w=3, d_h=1, d_w=1, name='conv7_conv2', w=gen_params["g_h%i_W" % 13], biases=gen_params["g_h%i_b" % 13])            conv7 = self.g_batch_norm["g_bn%i" % 13](conv7)            conv7 = tf.nn.relu(conv7, name='conv7_relu2')            conv7 = tf.nn.dropout(conv7, .5)            up3 = upsampling2d(conv7)  # up3 = tf_utils.upsampling3d(conv7, size=(2, 2, 2), name='conv8_up')            conv8 = tf.concat([up3, conv2], axis=3, name='conv8_concat')            conv8 = conv2d(conv8, 2 * self.gf_dim, k_h=3, k_w=3, d_h=1, d_w=1, name='conv8_conv1', w=gen_params["g_h%i_W" % 14], biases=gen_params["g_h%i_b" % 14])            conv8 = self.g_batch_norm["g_bn%i" % 14](conv8)            conv8 = tf.nn.relu(conv8, name='conv8_relu1')            conv8 = conv2d(conv8, 2 * self.gf_dim, k_h=3, k_w=3, d_h=1, d_w=1, name='conv8_conv2', w=gen_params["g_h%i_W" % 15], biases=gen_params["g_h%i_b" % 15])            conv8 = self.g_batch_norm["g_bn%i" % 15](conv8)            conv8 = tf.nn.relu(conv8, name='conv8_relu2')            up4 = upsampling2d(conv8)  # up4 = tf_utils.upsampling3d(conv8, size=(2, 2, 2), name='conv9_up')            conv9 = tf.concat([up4, conv1], axis=3, name='conv9_concat')            conv9 = conv2d(conv9, self.gf_dim, k_h=3, k_w=3, d_h=1, d_w=1, name='conv9_conv1', w=gen_params["g_h%i_W" % 16], biases=gen_params["g_h%i_b" % 16])            conv9 = self.g_batch_norm["g_bn%i" % 16](conv9)            conv9 = tf.nn.relu(conv9, name='conv9_relu1')            conv9 = conv2d(conv9, self.gf_dim, k_h=3, k_w=3, d_h=1, d_w=1, name='conv9_conv2', w=gen_params["g_h%i_W" % 17], biases=gen_params["g_h%i_b" % 17])            conv9 = self.g_batch_norm["g_bn%i" % 17](conv9)            conv9 = tf.nn.relu(conv9, name='conv9_relu2')            ################################################################################################################################################            # output layer: (N, 640, 640, 32) -> (N, 640, 640, 1)            logits = conv2d(conv9, 1, k_h=1, k_w=1, d_h=1, d_w=1, name='conv_output', w=gen_params["g_h%i_W" % 18], biases=gen_params["g_h%i_b" % 18])  #            #log_var = conv2d(conv9, 1, k_h=1, k_w=1, d_h=1, d_w=1, name='conv_output', w=gen_params["g_h%i_W" % 19], biases=gen_params["g_h%i_b" % 19])  #            ##################################################################################################################################################            return logits #, log_var  # , sigmoid # previous: tf.nn.sigmoid(output)    def res_generator(self, image, gen_params):        with tf.variable_scope("res_generator", reuse=tf.AUTO_REUSE) as scope:            shortcut1 = image            conv1 = conv2d(image, self.gf_dim, k_h=3, k_w=3, d_h=1, d_w=1, name='conv1_conv1',                           w=gen_params["g_h%i_W" % 0], biases=gen_params["g_h%i_b" % 0])            conv1 = self.g_batch_norm["g_bn%i" % 0](conv1)            conv1 = tf.nn.relu(conv1, name='conv1_relu1')            conv1 = conv2d(conv1, self.gf_dim, k_h=3, k_w=3, d_h=1, d_w=1, name='conv1_conv2',                           w=gen_params["g_h%i_W" % 1], biases=gen_params["g_h%i_b" % 1])            conv1 = self.g_batch_norm["g_bn%i" % 1](conv1)            if shortcut1.shape[3] != conv1.shape[3]:                shortcut1 = conv2d(shortcut1, conv1.shape[3], k_h=1, k_w=1, d_h=1, d_w=1, name='conv1_shortcut',                                   w=gen_params["g_h%i_W" % 20], biases=gen_params["g_h%i_b" % 20])            conv1_output = shortcut1 + conv1            conv1 = tf.nn.relu(conv1_output, name='conv2_relu2')            pool1 = maxpooling_2d(conv1, name='maxpool2')            #############################################            shortcut2 = pool1            conv2 = conv2d(pool1, 2 * self.gf_dim, k_h=3, k_w=3, d_h=1, d_w=1, name='conv2_conv1',                           w=gen_params["g_h%i_W" % 2], biases=gen_params["g_h%i_b" % 2])            conv2 = self.g_batch_norm["g_bn%i" % 2](conv2)            conv2 = tf.nn.relu(conv2, name='conv2_relu1')            conv2 = conv2d(conv2, 2 * self.gf_dim, k_h=3, k_w=3, d_h=1, d_w=1, name='conv2_conv2',                           w=gen_params["g_h%i_W" % 3], biases=gen_params["g_h%i_b" % 3])            conv2 = self.g_batch_norm["g_bn%i" % 3](conv2)            if shortcut2.shape[3] != conv2.shape[3]:                shortcut2 = conv2d(shortcut2, conv2.shape[3], k_h=1, k_w=1, d_h=1, d_w=1, name='conv2_shortcut',                           w=gen_params["g_h%i_W" % 21], biases=gen_params["g_h%i_b" % 21])            conv2_output = shortcut2 + conv2            conv2 = tf.nn.relu(conv2_output, name='conv2_relu2')            pool2 = maxpooling_2d(conv2, name='maxpool2')            #################################################            shortcut3 = pool2            conv3 = conv2d(pool2, 4 * self.gf_dim, k_h=3, k_w=3, d_h=1, d_w=1, name='conv3_conv1',                           w=gen_params["g_h%i_W" % 4], biases=gen_params["g_h%i_b" % 4])            conv3 = self.g_batch_norm["g_bn%i" % 4](conv3)            conv3 = tf.nn.relu(conv3, name='conv3_relu1')            conv3 = conv2d(conv3, 4 * self.gf_dim, k_h=3, k_w=3, d_h=1, d_w=1, name='conv3_conv2',                           w=gen_params["g_h%i_W" % 5], biases=gen_params["g_h%i_b" % 5])            conv3 = self.g_batch_norm["g_bn%i" % 5](conv3)            if shortcut3.shape[3] != conv3.shape[3]:                shortcut3 = conv2d(shortcut3, conv3.shape[3], k_h=1, k_w=1, d_h=1, d_w=1, name='conv3_shortcut',                           w=gen_params["g_h%i_W" % 22], biases=gen_params["g_h%i_b" % 22])            conv3_output = shortcut3 + conv3            conv3 = tf.nn.relu(conv3_output, name='conv3_relu2')            pool3 = maxpooling_2d(conv3, name='maxpool3')            pool3 = tf.nn.dropout(pool3, .5)            ######################################################            shortcut4 = pool3            conv4 = conv2d(pool3, 8 * self.gf_dim, k_h=3, k_w=3, d_h=1, d_w=1, name='conv4_conv1',                           w=gen_params["g_h%i_W" % 6], biases=gen_params["g_h%i_b" % 6])            conv4 = self.g_batch_norm["g_bn%i" % 6](conv4)            conv4 = tf.nn.relu(conv4, name='conv4_relu1')            conv4 = conv2d(conv4, 8 * self.gf_dim, k_h=3, k_w=3, d_h=1, d_w=1, name='conv4_conv2',                           w=gen_params["g_h%i_W" % 7], biases=gen_params["g_h%i_b" % 7])            conv4 = self.g_batch_norm["g_bn%i" % 7](conv4)            if shortcut4.shape[3] != conv4.shape[3]:                shortcut4 = conv2d(shortcut4, conv4.shape[3], k_h=1, k_w=1, d_h=1, d_w=1, name='conv4_shortcut',                           w=gen_params["g_h%i_W" % 23], biases=gen_params["g_h%i_b" % 23])            conv4_output = shortcut4 + conv4            conv4 = tf.nn.relu(conv4_output, name='conv4_relu2')            pool4 = maxpooling_2d(conv4, name='maxpool4')            pool4 = tf.nn.dropout(pool4, .5)            #############################################            shortcut5 = pool4            conv5 = conv2d(pool4, 16 * self.gf_dim, k_h=3, k_w=3, d_h=1, d_w=1, name='conv5_conv1',                           w=gen_params["g_h%i_W" % 8], biases=gen_params["g_h%i_b" % 8])            conv5 = self.g_batch_norm["g_bn%i" % 8](conv5)            conv5 = tf.nn.relu(conv5, name='conv5_relu1')            conv5 = conv2d(conv5, 16 * self.gf_dim, k_h=3, k_w=3, d_h=1, d_w=1, name='conv5_conv2',                           w=gen_params["g_h%i_W" % 9], biases=gen_params["g_h%i_b" % 9])            conv5 = self.g_batch_norm["g_bn%i" % 9](conv5)            if shortcut5.shape[3] != conv5.shape[3]:                shortcut5 = conv2d(shortcut5, conv5.shape[3], k_h=1, k_w=1, d_h=1, d_w=1, name='conv5_shortcut',                           w=gen_params["g_h%i_W" % 24], biases=gen_params["g_h%i_b" % 24])            conv5_output = shortcut5 + conv5            conv5 = tf.nn.relu(conv5_output, name='conv5_relu2')            conv5 = tf.nn.dropout(conv5, .5)            ################################################            up1 = upsampling2d(conv5)            conv6 = tf.concat([up1, conv4], axis=3, name='conv6_concat')  # axis = 3 ??            shortcut6 = conv6            conv6 = conv2d(conv6, 8 * self.gf_dim, k_h=3, k_w=3, d_h=1, d_w=1, name='conv6_conv1',                           w=gen_params["g_h%i_W" % 10], biases=gen_params["g_h%i_b" % 10])            conv6 = self.g_batch_norm["g_bn%i" % 10](conv6)            conv6 = tf.nn.relu(conv6, name='conv6_relu1')            conv6 = conv2d(conv6, 8 * self.gf_dim, k_h=3, k_w=3, d_h=1, d_w=1, name='conv6_conv2',                           w=gen_params["g_h%i_W" % 11], biases=gen_params["g_h%i_b" % 11])            conv6 = self.g_batch_norm["g_bn%i" % 11](conv6)            if shortcut6.shape[3] != conv6.shape[3]:                shortcut6 = conv2d(shortcut6, conv6.shape[3], k_h=1, k_w=1, d_h=1, d_w=1, name='conv6_shortcut',                           w=gen_params["g_h%i_W" % 25], biases=gen_params["g_h%i_b" % 25])            conv6_output = shortcut6 + conv6            conv6 = tf.nn.relu(conv6_output, name='conv6_relu2')            conv6 = tf.nn.dropout(conv6, .5)            ####################################################            up2 = upsampling2d(conv6)  # up2 = tf_utils.upsampling3d(conv6, size=(2, 2, 2), name='conv7_up')            conv7 = tf.concat([up2, conv3], axis=3, name='conv7_concat')            shortcut7 = conv7            conv7 = conv2d(conv7, 4 * self.gf_dim, k_h=3, k_w=3, d_h=1, d_w=1, name='conv7_conv1',                           w=gen_params["g_h%i_W" % 12], biases=gen_params["g_h%i_b" % 12])            conv7 = self.g_batch_norm["g_bn%i" % 12](conv7)            conv7 = tf.nn.relu(conv7, name='conv7_relu1')            conv7 = conv2d(conv7, 4 * self.gf_dim, k_h=3, k_w=3, d_h=1, d_w=1, name='conv7_conv2',                           w=gen_params["g_h%i_W" % 13], biases=gen_params["g_h%i_b" % 13])            conv7 = self.g_batch_norm["g_bn%i" % 13](conv7)            if shortcut7.shape[3] != conv7.shape[3]:                shortcut7 = conv2d(shortcut7, conv7.shape[3], k_h=1, k_w=1, d_h=1, d_w=1, name='conv7_shortcut',                           w=gen_params["g_h%i_W" % 26], biases=gen_params["g_h%i_b" % 26])            conv7_output = shortcut7 + conv7            conv7 = tf.nn.relu(conv7_output, name='conv7_relu2')            conv7 = tf.nn.dropout(conv7, .5)            ####################################################            up3 = upsampling2d(conv7)  # up3 = tf_utils.upsampling3d(conv7, size=(2, 2, 2), name='conv8_up')            conv8 = tf.concat([up3, conv2], axis=3, name='conv8_concat')            shortcut8 = conv8            conv8 = conv2d(conv8, 2 * self.gf_dim, k_h=3, k_w=3, d_h=1, d_w=1, name='conv8_conv1',                           w=gen_params["g_h%i_W" % 14], biases=gen_params["g_h%i_b" % 14])            conv8 = self.g_batch_norm["g_bn%i" % 14](conv8)            conv8 = tf.nn.relu(conv8, name='conv8_relu1')            conv8 = conv2d(conv8, 2 * self.gf_dim, k_h=3, k_w=3, d_h=1, d_w=1, name='conv8_conv2',                           w=gen_params["g_h%i_W" % 15], biases=gen_params["g_h%i_b" % 15])            conv8 = self.g_batch_norm["g_bn%i" % 15](conv8)            if shortcut8.shape[3] != conv8.shape[3]:                shortcut8 = conv2d(shortcut8, conv8.shape[3], k_h=1, k_w=1, d_h=1, d_w=1, name='conv8_shortcut',                           w=gen_params["g_h%i_W" % 27], biases=gen_params["g_h%i_b" % 27])            conv8_output = shortcut8 + conv8            conv8 = tf.nn.relu(conv8_output, name='conv8_relu2')            ####################################################            up4 = upsampling2d(conv8)  # up4 = tf_utils.upsampling3d(conv8, size=(2, 2, 2), name='conv9_up')            conv9 = tf.concat([up4, conv1], axis=3, name='conv9_concat')            shortcut9 = conv9            conv9 = conv2d(conv9, self.gf_dim, k_h=3, k_w=3, d_h=1, d_w=1, name='conv9_conv1',                           w=gen_params["g_h%i_W" % 16], biases=gen_params["g_h%i_b" % 16])            conv9 = self.g_batch_norm["g_bn%i" % 16](conv9)            conv9 = tf.nn.relu(conv9, name='conv9_relu1')            conv9 = conv2d(conv9, self.gf_dim, k_h=3, k_w=3, d_h=1, d_w=1, name='conv9_conv2',                           w=gen_params["g_h%i_W" % 17], biases=gen_params["g_h%i_b" % 17])            conv9 = self.g_batch_norm["g_bn%i" % 17](conv9)            if shortcut9.shape[3] != conv9.shape[3]:                shortcut9 = conv2d(shortcut9, conv9.shape[3], k_h=1, k_w=1, d_h=1, d_w=1, name='conv9_shortcut',                           w=gen_params["g_h%i_W" % 28], biases=gen_params["g_h%i_b" % 28])            conv9_output = shortcut9 + conv9            conv9 = tf.nn.relu(conv9_output, name='conv9_relu2')            ####################################################            # output layer: (N, 640, 640, 32) -> (N, 640, 640, 1)            logits = conv2d(conv9, 1, k_h=1, k_w=1, d_h=1, d_w=1, name='conv_output', w=gen_params["g_h%i_W" % 18], biases=gen_params["g_h%i_b" % 18])  ##            logits = tf.nn.relu(logits)            log_var = conv2d(conv9, 1, k_h=1, k_w=1, d_h=1, d_w=1, name='conv_output', w=gen_params["g_h%i_W" % 19],biases=gen_params["g_h%i_b" % 19])  ##            log_var = tf.nn.relu(log_var)            return logits, log_var  # , sigmoid # previous: tf.nn.sigmoid(output)    def gen_prior(self, gen_params):        with tf.variable_scope("generator") or tf.variable_scope("res_generator") as scope:            prior_loss = 0.0            for var in gen_params.values():                """                @var: has the shape as weights e.g. (3, 3, 1, 64)                @nn: has the shape the same as var, e.g. (3, 3, 1, 64)                @prior_loss: has the same shape as var, e.g. (3, 3, 1, 64)                 """                nn = tf.divide(var, self.prior_std)   # x^2 / sigma^2                prior_loss += tf.reduce_mean(tf.multiply(nn, nn))        prior_loss /= self.num_train        return prior_loss    def gen_noise(self, gen_params):        with tf.variable_scope("generator") or tf.variable_scope("res_generator") as scope:            noise_loss = 0.0            for name, var in gen_params.items():  # .iteritems():                """                @ name: g_h0_W                 @ var : shape with (3, 3, 1, 64)                 @ noise_sample : has the shape same as var (3, 3, 1, 64)                 @ var * noise.sample : has the shape same as var (3, 3, 1, 64)                 """                # Normal() returns a probability density function                noise_ = tf.distributions.Normal(loc=0., scale=self.noise_std * tf.ones(var.get_shape()))  # tf.contrib.distributions.Normal(mu=0., sigma=self.noise_std*tf.ones(var.get_shape()))                noise_loss += tf.reduce_sum(var * noise_.sample())   # stacked samples from the pdf        noise_loss /= self.num_train        return noise_loss